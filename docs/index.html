<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Deriving Rewards for Reinforcement Learning from Symbolic Behaviour Descriptions of Bipedal Walking</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Deriving Rewards for Reinforcement Learning from
Symbolic Behaviour Descriptions of Bipedal Walking</h1>


<p class="author">
  
  Daniel Harnack 
        <sup>1</sup>
          <a href="http://www.informatik.uni-bremen.de/~clueth/">
  
  Christoph LÃ¼th 
        <sup>1 2</sup>
        </a>
      <a href="https://robotik.dfki-bremen.de/de/ueber-uns/mitarbeiter/lugr02.html">
  
  Lukas Gross 
        <sup>1</sup>
        </a>
      <a href="https://robotik.dfki-bremen.de/de/ueber-uns/mitarbeiter/shku02.html">
  
  Shivesh Kumar 
        <sup>1</sup>
        </a>
      <a href="https://robotik.dfki-bremen.de/de/ueber-uns/mitarbeiter/frki01.html">
  
  Frank Kirchner 
        <sup>1 2</sup>
        </a>
  </p>

<p class="affiliation">

    <a href="https://www.dfki.de/">
  
  German Research Center for Artificial Intelligence 
      <sup>1</sup>
  
    </a>
  <br>
  
    <a href="https://www.uni-bremen.de/">
  
  University of Bremen 
      <sup>2</sup>
  
    </a>
  <br>
  </p>

</header>

<div class="header_links">
  <a href="https://github.com/dfki-ric-underactuated-lab/orthant_rewards_biped_rl/"><i class="fa fa-github" style="font-size:36px; padding:1em;"></i></a>
  <a href="http://arxiv.org/abs/2312.10328"><i class="fa fa-file-pdf-o" style="font-size:36px; padding:1em;"></i></a>
</div>
<h2 id="abstract">Abstract</h2>
<p>Generating physical movement behaviours from their symbolic
description is a long-standing challenge in artificial intelligence (AI)
and robotics, requiring insights into numerical optimization methods as
well as into formalizations from symbolic AI and reasoning. In this
paper, a novel approach to finding a reward function from a symbolic
description is proposed. The intended system behaviour is modelled as a
hybrid automaton, which reduces the system state space to allow more
efficient reinforcement learning. The approach is applied to bipedal
walking, by modelling the walking robot as a hybrid automaton over state
space orthants, and used with the compass walker to derive a reward that
incentivizes following the hybrid automaton cycle. As a result, training
times of reinforcement learning controllers are reduced while final
walking speed is increased. The approach can serve as a blueprint how to
generate reward functions from symbolic AI and reasoning.</p>
<h2 id="presentation">Presentation</h2>
<figure>
<embed src="static/slides.pdf" width="720" height="405" />
<figcaption aria-hidden="true">Presentation at the 62nd Conference on
Decision and Control (CDC 2023), Singapore, 13.12.2023</figcaption>
</figure>
<h2 id="video">Video</h2>
<div>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/CkvLvz_tLtc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
</div>
<h2 id="code">Code</h2>
<p>The source code for the work described in the paper can be found <a
href="https://github.com/dfki-ric-underactuated-lab/orthant_rewards_biped_rl/">here</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@inproceedings{HLGKK23,
  author = { Harnack,  Daniel and
             L\&quot;{u}th, Christoph and
             Gross, Lukas and
             Kumar, Shivesh and 
             Kirchner, Frank}
  title =  { Deriving Rewards for Reinforcement Learning from Symbolic Behaviour Descriptions of Bipedal Walking},
  booktitle = {62nd {IEEE} Conference on Decision and Control ({CDC})},
  address   = {Marina Bay Sands, Singapore} 
  pages     = {2135 -- 2140},
  year      = {2023},
  publisher = {{IEEE}}
}</code></pre>
<footer>
  <img src="static/DFKI-Logo.png" style="height:60px">
  <img src="static/ulab.gif" style="height:60px">
  <img src="static/veryhuman.jpg" style="height:60px">
</footer>
</body>
</html>
